{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 11:56:44.537440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 11:56:44.552661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 11:56:44.557473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 11:56:44.569685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 11:56:45.785684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading cached shuffled indices for dataset at /tmp/tmpwct5z_5f/transformer_train_dringlichkeit_hgdataset/cache-a0731bbdc9537f86.arrow\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25785dcb9e9844a2b2a4fdc9dd480fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Train Dataset:   0%|          | 0/32640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987b84d8f63f420699db8779ba260c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Test Dataset:   0%|          | 0/8160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6121' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6121/20400 1:04:19 < 2:30:07, 1.59 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Fnr</th>\n",
       "      <th>Tnr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>0.632186</td>\n",
       "      <td>0.661152</td>\n",
       "      <td>0.857551</td>\n",
       "      <td>0.373350</td>\n",
       "      <td>0.520215</td>\n",
       "      <td>0.626650</td>\n",
       "      <td>0.939928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.630026</td>\n",
       "      <td>0.672059</td>\n",
       "      <td>0.757797</td>\n",
       "      <td>0.490162</td>\n",
       "      <td>0.595281</td>\n",
       "      <td>0.509838</td>\n",
       "      <td>0.848251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>0.594276</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.917170</td>\n",
       "      <td>0.377833</td>\n",
       "      <td>0.535191</td>\n",
       "      <td>0.622167</td>\n",
       "      <td>0.966948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Oct 21 15:03:07 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Oct 21 15:03:07 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Oct 21 15:03:07 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#Datensatz laden und aufteilen\n",
    "dataset = load_from_disk(\"transformer_train_dringlichkeit_hgdataset\")\n",
    "#dataset = load_from_disk(\"transformer_train_NLP_dringlichkeit_hgdataset\")\n",
    "# normaler Datensatz f√ºr das Basisverfahren\n",
    "# NLP-Datensatz f√ºr das erweiterte und Hyperparametergest√ºtzte Verfahren\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "train_test_split = shuffled_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "#Debugging fehlendes Padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#Modellauswahl\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#Textvorverarbeitung f√ºr das Modelltraining\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"message\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    tokenized_examples[\"labels\"] = examples[\"class\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing Train Dataset\",\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing Test Dataset\",\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')\n",
    "\n",
    "#Trainingsargumente\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mdeberta_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    #gradient_accumulation_steps=3,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),  \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#Berechnung der Bewertungsmetriken\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"fnr\": fnr,\n",
    "        \"tnr\": tnr,\n",
    "    }\n",
    "\n",
    "#CustomCallback um die Bewertungsmetriken pro Epoche zu sichern\n",
    "class SaveMetricsCallback(TrainerCallback):\n",
    "    def __init__(self, output_file=\"mdeberta_results/metrics.xlsx\"):\n",
    "        self.metrics = []\n",
    "        self.output_file = output_file\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Save metrics\n",
    "        metrics[\"epoch\"] = state.epoch\n",
    "        self.metrics.append(metrics.copy())\n",
    "        # Save to Excel file\n",
    "        df = pd.DataFrame(self.metrics)\n",
    "        df.to_excel(self.output_file, index=False)\n",
    "\n",
    "#Trainer definieren\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[SaveMetricsCallback(output_file=\"mdeberta_results/metrics.xlsx\")],\n",
    ")\n",
    "\n",
    "#Training starten\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
